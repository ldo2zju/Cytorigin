{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b65d5218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\woloo\\anaconda3\\envs\\scVAEDer\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import umap.umap_ as umap\n",
    "import random\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9403a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_h5ad = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4901bec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read(path_to_h5ad)\n",
    "adata = adata[adata.obs[\"stage\"].isin([\"E12\",\"CS7\"])]\n",
    "#adata = adata[random.sample(adata.obs_names.to_list(), 6000)] # if exceeds 6,000 cells\n",
    "\n",
    "sc.pp.filter_cells(adata, min_genes=200)\n",
    "sc.pp.filter_genes(adata, min_cells=30)\n",
    "sc.pp.normalize_per_cell(adata)\n",
    "sc.pp.log1p(adata)\n",
    "sc.pp.highly_variable_genes(adata, n_top_genes=8000)\n",
    "adata = adata[:, adata.var['highly_variable']]\n",
    "\n",
    "data=pd.DataFrame(adata.X.todense())\n",
    "data = data.to_numpy(dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa86165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[:1100]\n",
    "test_data = data[1100:]\n",
    "\n",
    "# Convert data to PyTorch tensors and create data loaders\n",
    "train_data = torch.Tensor(train_data)\n",
    "test_data = torch.Tensor(test_data)\n",
    "train_loader = DataLoader(TensorDataset(train_data), batch_size=200, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(test_data), batch_size=200, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ccb99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "laten_size=30   #Size of the latent layer\n",
    "layer1=100   #Size of the first layer of enocder and decoder\n",
    "input_size=8000\n",
    "\n",
    "# Define the variational autoencoder model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, layer1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer1, laten_size),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(laten_size, layer1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(layer1, input_size),\n",
    "        )\n",
    "        self.mu = nn.Linear(laten_size, laten_size)\n",
    "        self.log_var = nn.Linear(laten_size, laten_size)\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu = self.mu(x)\n",
    "        log_var = self.log_var(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x = self.decoder(z)\n",
    "        return x, mu, log_var\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        z = torch.randn(num_samples, laten_size)\n",
    "        return self.decoder(z)\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = VAE()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the loss function: We also tried different loss functions Binary Cross-Entropy (BCE) Loss for \n",
    "# the reconstuciton loss of VAE but MSE worked better (lower error):\n",
    "def vae_loss(x, x_recon, mu, log_var):\n",
    "    recon_loss = nn.functional.mse_loss(x_recon, x, reduction='sum')\n",
    "    kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return recon_loss + kl_div\n",
    "\n",
    "# Train the model\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "num_epochs = 150\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        x = batch[0]\n",
    "        x_recon, mu, log_var = model(x)\n",
    "        loss = vae_loss(x, x_recon, mu, log_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_losses.append(train_loss / len(train_data))\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            x = batch[0]\n",
    "            x_recon, mu, log_var = model(x)\n",
    "            loss = vae_loss(x, x_recon, mu, log_var)\n",
    "            test_loss += loss.item()\n",
    "        test_losses.append(test_loss / len(test_data))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}\")\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Convert the data to a PyTorch tensor\n",
    "data_tensor = torch.tensor(data).float()\n",
    "\n",
    "# Compute the latent layer for the data\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    latent_layer = model.encoder(data_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b13bdb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_layer = (latent_layer - latent_layer.mean())/(latent_layer.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e8715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the latent layer using UMAP\n",
    "umap_embedding = umap.UMAP(n_neighbors=100, min_dist=0.3, random_state=42).fit_transform(latent_layer)\n",
    "plt.scatter(umap_embedding[:, 0], umap_embedding[:, 1], c=numeric_labels, s=5, cmap='viridis')\n",
    "plt.show()\n",
    "dataset = torch.Tensor(latent_layer).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484d811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_beta_schedule(schedule='linear', num_steps=1000, start=1e-5, end=1e-2):\n",
    "    if schedule == 'linear':\n",
    "        betas = torch.linspace(start, end, num_steps)\n",
    "    elif schedule == \"quad\":\n",
    "        betas = torch.linspace(start ** 0.5, end ** 0.5, num_steps) ** 2\n",
    "    elif schedule == \"sigmoid\":\n",
    "        betas = torch.linspace(-6, 6, num_steps)\n",
    "        betas = torch.sigmoid(betas) * (end - start) + start\n",
    "    return betas\n",
    "\n",
    "def extract(input, t, x):\n",
    "    shape = x.shape\n",
    "    out = torch.gather(input, 0, t.to(input.device))\n",
    "    reshape = [t.shape[0]] + [1] * (len(shape) - 1)\n",
    "    return out.reshape(*reshape)\n",
    "\n",
    "def plot_schedule(num_steps,schedule):\n",
    "    plt.plot(list(range(num_steps)),betas.numpy(),label='betas')\n",
    "    plt.plot(list(range(num_steps)),torch.sqrt(alphas_prod).numpy(),label='sqrt_alphas_prod')\n",
    "    plt.plot(list(range(num_steps)),torch.sqrt(1-alphas_prod).numpy(),label='sqrt_one_minus_alphas_prod')\n",
    "    plt.legend(['betas','sqrt_alphas_prod','sqrt_one_minus_alphas_prod'],loc = 'upper left')\n",
    "    plt.xlabel('steps')\n",
    "    plt.ylabel('value')\n",
    "    plt.title('{} schedule'.format(schedule))\n",
    "    plt.show()\n",
    "\n",
    "num_steps=1000\n",
    "\n",
    "schedule='sigmoid'\n",
    "betas = make_beta_schedule(schedule=schedule, num_steps=num_steps, start=1e-5, end=1e-2)\n",
    "alphas = 1-betas\n",
    "alphas_prod = torch.cumprod(alphas,0)\n",
    "alphas_prod_p = torch.cat([torch.tensor([1]).float(),alphas_prod[:-1]],0)\n",
    "alphas_bar_sqrt = torch.sqrt(alphas_prod)\n",
    "one_minus_alphas_bar_log = torch.log(1 - alphas_prod)\n",
    "one_minus_alphas_bar_sqrt = torch.sqrt(1 - alphas_prod)\n",
    "plot_schedule(num_steps,schedule)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7d0b1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_x(x_0,t):\n",
    "    noise = torch.randn_like(x_0).to(device)\n",
    "    alphas_t = alphas_bar_sqrt[t].to(device)\n",
    "    alphas_1_m_t = one_minus_alphas_bar_sqrt[t].to(device)\n",
    "    return (alphas_t * x_0 + alphas_1_m_t * noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d6caa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPDiffusion(nn.Module):\n",
    "    def __init__(self,n_steps, num_units=512):\n",
    "        super(MLPDiffusion,self).__init__()\n",
    "        \n",
    "        self.linears = nn.ModuleList(\n",
    "            [\n",
    "                nn.Linear(laten_size,num_units),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(num_units,num_units),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(num_units,num_units),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(num_units,laten_size),\n",
    "            ]\n",
    "        )\n",
    "        self.step_embeddings = nn.ModuleList(\n",
    "            [\n",
    "                nn.Embedding(n_steps,num_units),\n",
    "                nn.Embedding(n_steps,num_units),\n",
    "                nn.Embedding(n_steps,num_units),\n",
    "            ]\n",
    "        )\n",
    "    def forward(self,x,t):\n",
    "#         x = x_0\n",
    "        for idx,embedding_layer in enumerate(self.step_embeddings):\n",
    "            t_embedding = embedding_layer(t)\n",
    "            x = self.linears[2*idx](x)\n",
    "            x += t_embedding\n",
    "            x = self.linears[2*idx+1](x)\n",
    "            \n",
    "        x = self.linears[-1](x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f28e4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffusion_loss_fn(model, x_0, alphas_bar_sqrt, one_minus_alphas_bar_sqrt, n_steps):\n",
    "    \"\"\"Sampling and calculating loss at any time t\"\"\"\n",
    "    batch_size = x_0.shape[0]\n",
    "    \n",
    "    half_size = batch_size // 2\n",
    "    if batch_size % 2 == 1:\n",
    "\n",
    "        half_size = (batch_size + 1) // 2\n",
    "    \n",
    "    t = torch.randint(0, n_steps, size=(half_size,), device=x_0.device)\n",
    "    t = torch.cat([t, n_steps-1-t], dim=0)\n",
    "    \n",
    "    t = t[:batch_size].unsqueeze(-1)\n",
    "    \n",
    "    a = alphas_bar_sqrt[t]\n",
    "    aml = one_minus_alphas_bar_sqrt[t]\n",
    "\n",
    "    e = torch.randn_like(x_0, device=x_0.device)\n",
    "    \n",
    "    x = (x_0 * a + e * aml).to(device)\n",
    "    \n",
    "    output = model(x, t.squeeze(-1))\n",
    "    \n",
    "    return (e - output).square().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0702514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_sample_loop(model, shape, n_steps, betas, one_minus_alphas_bar_sqrt):\n",
    "    \"\"\"Restore x[T-1], x[T-2]|...x[0] from x[T]\"\"\"\n",
    "    cur_x = torch.randn(shape).to(device)\n",
    "    x_seq = [cur_x]\n",
    "    for i in reversed(range(n_steps)):\n",
    "        cur_x = p_sample(model, cur_x, i, betas, one_minus_alphas_bar_sqrt).to(device)\n",
    "        x_seq.append(cur_x)\n",
    "    return x_seq\n",
    "\n",
    "\n",
    "def p_sample(model, x, t, betas, one_minus_alphas_bar_sqrt):\n",
    "    \"\"\"Sampling the reconstructed value at time t from x[T]\"\"\"\n",
    "    device=\"cpu\"\n",
    "    t = torch.tensor([t]).to(device)\n",
    "    betas = betas.to(device)\n",
    "    one_minus_alphas_bar_sqrt = one_minus_alphas_bar_sqrt.to(device)\n",
    "    coeff = (betas[t] / one_minus_alphas_bar_sqrt[t]).to(device)\n",
    "\n",
    "    eps_theta = model(x, t).to(device)\n",
    "\n",
    "    mean = (1 / (1 - betas[t]).sqrt()) * (x - (coeff * eps_theta)).to(device)\n",
    "\n",
    "    z = torch.randn_like(x).to(device)\n",
    "    sigma_t = betas[t].sqrt().to(device)\n",
    "\n",
    "    sample = mean + sigma_t * z\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4bc52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA():\n",
    "    def __init__(self,mu=0.001):\n",
    "        self.mu = mu\n",
    "        self.shadow = {}\n",
    "        \n",
    "    def register(self,name,val):\n",
    "        self.shadow[name] = val.clone()\n",
    "        \n",
    "    def __call__(self,name,x):\n",
    "        assert name in self.shadow\n",
    "        new_average = self.mu * x + (1.0-self.mu)*self.shadow[name]\n",
    "        self.shadow[name] = new_average.clone()\n",
    "        return new_average\n",
    "\n",
    "device=\"cuda\"\n",
    "total_loss=[]\n",
    "print('Training model...')\n",
    "batch_size = 1200\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "num_epoch = 1500\n",
    "plt.rc('text',color='blue')\n",
    "model2 = MLPDiffusion(num_steps)\n",
    "model2.to(device)\n",
    "model2 = torch.nn.DataParallel(model2).to(device)\n",
    "alphas_bar_sqrt = alphas_bar_sqrt.to(device)\n",
    "one_minus_alphas_bar_sqrt = one_minus_alphas_bar_sqrt.to(device)\n",
    "optimizer = torch.optim.Adam(model2.parameters(),lr=1e-3)\n",
    "\n",
    "for t in range(num_epoch):\n",
    "    print(\"The values are: {} and {}\".format(loss, t))\n",
    "    for idx,batch_x in enumerate(dataloader):\n",
    "        batch_x = batch_x.to(device)\n",
    "        loss = diffusion_loss_fn(model2, batch_x, alphas_bar_sqrt, one_minus_alphas_bar_sqrt, num_steps)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model2.parameters(),1.)\n",
    "        optimizer.step()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e115ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# Specify a path to save to\n",
    "PATH = \"model_interpolate.pth\" # Choose whatever you like\n",
    "# Save\n",
    "torch.save(model2.state_dict(), PATH)\n",
    "# Load\n",
    "device = torch.device('cpu')\n",
    "model4 = MLPDiffusion(num_steps)\n",
    "\n",
    "# Original saved file with DataParallel\n",
    "state_dict = torch.load(\"model_interpolate.pth\")\n",
    "# create new OrderedDict that does not contain `module.`\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:] # remove `module.`\n",
    "    new_state_dict[name] = v\n",
    "# load params\n",
    "model4.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0978f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_sample_loop_optimized(model, shape, n_steps, betas, one_minus_alphas_bar_sqrt):\n",
    "    cur_x = torch.randn(shape)\n",
    "\n",
    "    for i in reversed(range(n_steps)):\n",
    "        cur_x = p_sample(model, cur_x, i, betas, one_minus_alphas_bar_sqrt)\n",
    "        print(i)\n",
    "    return cur_x \n",
    "\n",
    "x_final = p_sample_loop_optimized(model4, torch.Size([1000, 30]), num_steps, betas, one_minus_alphas_bar_sqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319e9f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_x = x_final.detach().cpu().numpy()\n",
    "umap_emb = umap.UMAP(n_neighbors=100, min_dist=0.2).fit_transform(cur_x)\n",
    "plt.scatter(umap_emb[:, 0], umap_emb[:, 1], s=10);  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c5eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_expr = model.decoder(cur_x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scVAEDer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
